{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive Only Sparse Coding on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements Positive Only Sparse Coding on MNIST.\n",
    "\n",
    "Lets break that down.\n",
    "\n",
    "Sparse Coding is an algorithm that seeks to disentangle the underlying generative factors of data. In this particular case, we are considering the dataset, MNIST, which is a collection of digits. Intuitively speaking, the underlying generative factors of digits are pen strokes. When these strokes are combined in the right way, we get digits. We will see that sparse coding discovers this underlying structure. \n",
    "\n",
    "In sparse coding, we define the following:\n",
    "$I$ is an image, $D$ is a dictionary of underlying generative factors, and $A$ the sparse coefficients. In long form, \n",
    "\n",
    "$$I(x) = d_1(x) * a_1 + d_2(x) * a_2 + \\ldots$$\n",
    "\n",
    "where the dictionary elements are the same for any image in the data set, and the coefficients, $a$, are different for each image. \n",
    "\n",
    "In order to learn the dictionary elements $D$, and the sparse coefficients, $A$, we minimize the following objective function:\n",
    "\n",
    "$$E = |I - A * D| ^ 2 + |A|_1 \\qquad \\sum_x d_i(x)^2 = 1$$\n",
    "\n",
    "(Note $D$ is number of dictionary elements by the number of pixels). To minimize this function, we do the following steps:\n",
    "\n",
    "0. Choose an initial value of $D$.\n",
    "\n",
    "1. Choose a batch of images $I$.\n",
    "\n",
    "2. Minimize $E$ with respect to $A$ - we use FISTA which is a slighly better version of gradient descent.\n",
    "\n",
    "3. Keeping that value of $A$, reduce $E$ by one gradient step in the direction of $\\nabla_D E$. \n",
    "\n",
    "4. Return to step 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.io import savemat, loadmat\n",
    "from scipy.linalg import eigh\n",
    "from scipy.stats import probplot, expon\n",
    "from sklearn.preprocessing import normalize\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from utils.rf_plot import show_fields\n",
    "from utils.fista import fista_updates, ista_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "data_dir = \"data/final/\"\n",
    "data_fil = \"mnist.mat\"\n",
    "output_dir = 'output/'\n",
    "\n",
    "data = loadmat(data_dir + data_fil)\n",
    "IMAGES = data['IMAGES']\n",
    "LABELS = data['LABELS']\n",
    "\n",
    "pos_only = True # Positive Only Sparse coding if True\n",
    "    \n",
    "# Scale Images to have equal standard deviations and pixels between 0 and 1\n",
    "K = IMAGES.shape[0]\n",
    "IMAGES = IMAGES / np.std(IMAGES.astype('float32'), axis = (1, 2), keepdims = True)\n",
    "IMAGES = IMAGES / np.max(IMAGES).astype('float32')\n",
    "\n",
    "# Set basic parameters\n",
    "(K, L_img, L_img) = IMAGES.shape  \n",
    "K # Number of base images\n",
    "L_img # Linear size of images from the data\n",
    "N_pix_img = L_img ** 2\n",
    "\n",
    "L_pat = L_img  # Size of image patch\n",
    "N_pix = L_pat ** 2  # Number of pixels in a patch\n",
    "N_sp = 81 # Number of sparse dictionary elements\n",
    "N_bat = 200 # Number of images in a batch\n",
    "Alpha = 0.01 # coefficient for the sparse coefficients\n",
    "Eta = 0.01 # Dictionary Learning Step Size (annealed, see later)\n",
    "\n",
    "D_std = np.std(IMAGES) # Standard deviation for dictionary elements\n",
    "D = np.zeros((N_sp, N_pix), dtype = 'float32') # Dictionary\n",
    "D[:, :] = np.random.uniform(size=D.shape) # Randomly initialize the dictionary\n",
    "D = D_std * normalize(D, axis = 1, norm = 'l2') # Normalize dictionary elements\n",
    "A = np.zeros((N_bat, N_sp), dtype = 'float32') # Initialize sparse coefficients\n",
    "# I_bat = np.zeros((N_bat, N_pix), dtype = 'float32') # Image batch\n",
    "# I - A * D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the derivatives necessary to do the minimization, we use a popular python package that is popular in the machine learning called theano. The idea is that we build up our objective function in terms of symbolic variables, and then we can use theano to automatically calculate the derivatives for us. Also theano allows for us to run the code on either a CPU or a GPU without changing our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def threshold(t_X):\n",
    "    \"\"\"\n",
    "    Threshold function\n",
    "    \"\"\"\n",
    "    return T.switch(t_X > 0., t_X, -0.00 * t_X )\n",
    "\n",
    "t_Alpha = T.scalar('Alpha')\n",
    "t_L = T.scalar('L')\n",
    "t_Eta = T.scalar('eta')\n",
    "t_D_std = T.scalar('D_std')\n",
    "\n",
    "t_I = theano.shared(IMAGES.reshape(K, N_pix_img).astype('float32'), 'I')\n",
    "t_D = theano.shared(D, 'D')\n",
    "t_A = theano.shared(A, 'A')\n",
    "t_I_idx = T.ivector('I_idx') # Indicies into t_I to select a batch of images\n",
    "\n",
    "t_E_rec = T.sum((t_I[t_I_idx] - T.dot(t_A, t_D)) ** 2); t_E_rec.name = 'E_rec'\n",
    "t_E_sp = t_Alpha * T.sum(T.abs_(t_A)); t_E_sp.name = 'E_sp'\n",
    "t_E = t_E_rec + t_E_sp; t_E.name = 'E'\n",
    "\n",
    "t_Signal = T.sum(t_I[t_I_idx] ** 2)\n",
    "t_SNR = t_Signal / t_E_rec # FIXME: Not quite right\n",
    "\n",
    "t_gED = T.grad(t_E_rec, t_D)\n",
    "\n",
    "costs = theano.function(inputs = [t_I_idx, t_Alpha],\n",
    "                        outputs = [t_E, t_E_rec, t_E_sp, t_SNR])\n",
    "\n",
    "def row_norm(t_X, t_std = t_D_std):\n",
    "    \"\"\"\n",
    "    Returns row normalized version of a theano matrix t_X\n",
    "    the rows have a norm of t_std\n",
    "    \"\"\"\n",
    "    return t_std * t_X / T.sqrt(0.000001 + T.sum(t_X ** 2, axis = 1)).dimshuffle(0, 'x')\n",
    "\n",
    "dictionary_learning_step = theano.function(\n",
    "    inputs = [t_Alpha, t_Eta, t_D_std, t_I_idx],\n",
    "    outputs = [t_E, t_E_rec, t_E_sp],\n",
    "    updates = [(t_D, row_norm(threshold(t_D - t_Eta * row_norm(t_gED))))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fist_updates = fista_updates(t_A, t_E_rec, t_Alpha, t_L)\n",
    "_, t_fista_X, t_T = fist_updates.keys()\n",
    "\n",
    "fista_step = theano.function(inputs = [t_Alpha, t_L, t_I_idx],\n",
    "                             outputs = [t_E, t_E_rec, t_E_sp, t_SNR],\n",
    "                             updates = fist_updates)\n",
    "def calculate_fista_L():\n",
    "    \"\"\"\n",
    "    Calculates the 'L' constant for FISTA for the dictionary in t_D.get_value()\n",
    "    \"\"\"\n",
    "    D = t_D.get_value()\n",
    "    try:\n",
    "        L = (2 * eigh(np.dot(D, D.T), eigvals_only=True, eigvals=(N_sp-1,N_sp-1))[0]).astype('float32')\n",
    "    except ValueError:\n",
    "        L = (2 * D_std ** 2 * N_sp).astype('float32') # Upper bound on largest eigenvalue\n",
    "    return L\n",
    "\n",
    "def reset_fista_variables():\n",
    "    \"\"\"\n",
    "    Resets fista variables\n",
    "    \"\"\"\n",
    "    A0 = np.zeros_like(t_A.get_value()).astype(theano.config.floatX)\n",
    "    t_A.set_value(A0)\n",
    "    t_fista_X.set_value(A0)\n",
    "    t_T.set_value(np.array([1.]).astype(theano.config.floatX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(N_itr, Alpha, Eta, cost_list, N_g_itr = 200, show_costs = True):\n",
    "    \"\"\"\n",
    "    Code to train a sparse coding dictionary\n",
    "    N_itr - Number of iterations, a new batch of images for each\n",
    "    Alpha - Sparsity cost parameter... E = E_rec + Alpha + |A|_1\n",
    "    cost_list - list to which the cost at each iteration will be appended\n",
    "    N_g_itr - number of gradient steps in FISTA\n",
    "    show_costs - If true, print out costs every N_itr/10 iterations\n",
    "    Returns I_idx - indices corresponding to the most recent image batch,\n",
    "        to be used later in visualizations\n",
    "    \"\"\"\n",
    "    if show_costs:\n",
    "        print 'Iteration, E, E_rec, E_sp, SNR'\n",
    "    for i in range(N_itr):\n",
    "        I_idx = np.random.randint(K, size = N_bat).astype('int32')\n",
    "        reset_fista_variables()\n",
    "        L = calculate_fista_L()\n",
    "        for _ in range(N_g_itr):\n",
    "            E, E_rec, E_sp, SNR = fista_step(Alpha, L, I_idx)\n",
    "        dictionary_learning_step(Alpha, Eta, D_std, I_idx)\n",
    "        # \n",
    "        E, E_rec, E_sp, SNR = costs(I_idx, Alpha)\n",
    "        cost_list.append(E)\n",
    "        if ((i + 1) % (1 + N_itr / 10) == 0) and show_costs:\n",
    "            print i, E, E_rec, E_sp, SNR\n",
    "    return I_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I_idx = train(150, Alpha, 0.05, cost_list, show_costs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_idx = train(50, Alpha, 0.01, cost_list, show_costs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_idx = train(100, Alpha, 0.005, cost_list, show_costs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_idx = train(1, Alpha, 0.001, [], show_costs=False, N_g_itr=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(np.array(cost_list)))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log(cost)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "show_fields(t_D.get_value(), cmap = plt.cm.gray, pos_only = True)\n",
    "#plt.savefig(output_dir + 'mnist_basis.png', dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "q = np.random.randint(N_bat)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Reconstruction')\n",
    "plt.imshow(np.dot(t_A.get_value(), t_D.get_value())[q].reshape(L_pat, L_pat),\n",
    "           interpolation = 'nearest',\n",
    "           cmap = plt.cm.gray, vmin = 0, vmax = t_I.get_value()[I_idx][q].reshape(L_pat, L_pat).max())\n",
    "plt.colorbar()\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Orignal Image')\n",
    "plt.imshow(t_I.get_value()[I_idx][q].reshape(L_pat, L_pat),\n",
    "           interpolation = 'nearest',\n",
    "           cmap = plt.cm.gray)\n",
    "plt.colorbar()\n",
    "plt.subplot(2, 2, 3)\n",
    "\n",
    "plt.hist(t_A.get_value()[q])\n",
    "\n",
    "sort_idx = np.argsort(t_A.get_value()[q])[::-1]\n",
    "N_active = np.sum(t_A.get_value()[q] > 0.0)\n",
    "active_idx = sort_idx[0:N_active]\n",
    "\n",
    "plt.title('Histogram of sparse Coefficients: \\n Number of active coefficients %d' % N_active)\n",
    "plt.xlabel('Coefficient Activity')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "show_fields(t_D.get_value()[active_idx] * \n",
    "            t_A.get_value()[q][active_idx][:, np.newaxis], \n",
    "            cmap = plt.cm.gray, pos_only = True)\n",
    "plt.title('Active Dictionary Elements \\n Scaled by their activations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "As = t_A.get_value().ravel()\n",
    "probplot(As[As > 0], dist = expon, plot = plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of FISTA, ISTA, and ADADelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check our implementation of FISTA, we compare it to the convergence of ISTA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ist_updates = ista_updates(t_A, t_E_rec, t_Alpha, t_L)\n",
    "\n",
    "ista_step = theano.function(inputs = [t_Alpha, t_L, t_I_idx],\n",
    "                             outputs = [t_E, t_E_rec, t_E_sp, t_SNR],\n",
    "                             updates = ist_updates)\n",
    "def reset_ista_variables():\n",
    "    \"\"\"\n",
    "    Resets fista variables\n",
    "    \"\"\"\n",
    "    A0 = np.zeros_like(t_A.get_value()).astype(theano.config.floatX)\n",
    "    t_A.set_value(A0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.theano_gradient_routines import ada_delta\n",
    "t_Rho = T.scalar('Rho')\n",
    "t_Eps = T.scalar('Eps')\n",
    "ada_updates = ada_delta(t_E, t_A, *(t_Rho, t_Eps))\n",
    "t_ada_Eg2, t_ada_dA2, _ = ada_updates.keys()\n",
    "\n",
    "ada_delta_step = theano.function(inputs = [t_Alpha, t_I_idx, t_Rho, t_Eps],\n",
    "                                 outputs = [t_E, t_E_rec, t_E_sp, t_SNR],\n",
    "                                 updates = ada_updates)\n",
    "\n",
    "def reset_adadelta_variables():\n",
    "    \"\"\"\n",
    "    Resets ADA Delta auxillary variables\n",
    "    \"\"\"\n",
    "    A0 = np.zeros_like(t_A.get_value()).astype(theano.config.floatX)\n",
    "    t_ada_Eg2.set_value(A0)\n",
    "    t_ada_dA2.set_value(A0)\n",
    "    t_A.set_value(A0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_bat = 200\n",
    "t_A.set_value(np.zeros((N_bat, N_sp)).astype(theano.config.floatX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = (calculate_fista_L()).astype('float32')\n",
    "I_idx = np.random.randint(K, size = N_bat).astype('int32')\n",
    "N_g_itr = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ada_cost_list = []\n",
    "reset_adadelta_variables()\n",
    "Eps = 0.1\n",
    "Rho = 0.8\n",
    "for _ in range(N_g_itr):\n",
    "    E, E_rec, E_sp, SNR = ada_delta_step(Alpha, I_idx, Rho, Eps)\n",
    "    ada_cost_list.append(E)\n",
    "ada_cost_array = np.array(ada_cost_list) / N_bat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "As = t_A.get_value().ravel()\n",
    "probplot(As[As > 0], dist = expon, plot = plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fista_cost_list = []\n",
    "reset_fista_variables()\n",
    "for _ in range(N_g_itr):\n",
    "    E, E_rec, E_sp, SNR = fista_step(Alpha, L, I_idx)\n",
    "    fista_cost_list.append(E)\n",
    "fista_cost_array = np.array(fista_cost_list) / N_bat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ista_cost_list = []\n",
    "reset_ista_variables()\n",
    "for _ in range(N_g_itr):\n",
    "    E, E_rec, E_sp, SNR = ista_step(Alpha, L, I_idx)\n",
    "    ista_cost_list.append(E)\n",
    "ista_cost_array = np.array(ista_cost_list) / N_bat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = np.min([np.min(fista_cost_array), \n",
    "            np.min(ista_cost_array),\n",
    "           np.min(ada_cost_array)])\n",
    "c0 = fista_cost_array[0]\n",
    "x_itr = 1 + np.arange(N_g_itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the convergence for ISTA and FISTA. Suppose we are trying to minimize $F(x)$ and $x^*$ is $\\text{argmin}_x F(x)$. If $k$ is the iteration number, then the theoretical bounds say that the error for ISTA scales as $O(1/k)$ and the error for FISTA scales as $O(1/k^2)$. The dotted lines show these two scalings. We can see roughly that the FISTA error scales as we want, as does the ISTA error. However, after about $100$ iterations, both functions start scaling down faster. This might be due to the lack of complexity of this minimization problem or the fact that we are doing float32 computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x_itr, (fista_cost_array - m)/c0, label = 'FISTA')\n",
    "plt.plot(x_itr, (ista_cost_array - m)/c0, label = 'ISTA')\n",
    "plt.plot(x_itr, (ada_cost_array - m) /c0, label = 'ADADelta')\n",
    "plt.plot(x_itr, 1./x_itr, label='1/k', ls='--')\n",
    "plt.plot(x_itr, 1./x_itr**2, label='1/k^2', ls='--')\n",
    "plt.legend(loc = 0)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('(F(x_t) - F(x*))/F(x_0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details on FISTA:\n",
    "\n",
    "Since the objective function has an absolute value, typical gradient descent approaches converge slowly. Thus there are special purpose gradient descent methods that minimize functions that are in the form $$f(x) + g(x)$$ where $f(x)$ is a continuously differentiable, convex function and $g(x)$ is a convex, but not continuously differentiable function, such as $g(x) = \\alpha |x|$. One such method is called FISTA, or the Fast Iterative Shrinkage-Threshold Algorithm. \n",
    "\n",
    "The core kernel of the FISTA algorithm is the ISTA step:\n",
    "\n",
    "Define\n",
    "$$p_L(y) = \\text{argmin}_x \\, g(x) + L/2 * ||x- g(y)||^2$$ where $$g(y) = y - \\frac{1}{L} \\nabla f(y)$$\n",
    "\n",
    "and where $L$ is the constant such that $$||\\nabla f(x) - \\nabla f(y)|| \\le L ||x - y||$$\n",
    "\n",
    "When $g(x) = \\alpha|x|_1$, then $$p_L(y) = h_\\theta(g(y))\\qquad h_\\theta(y) = \\text{sign}(y)(|y|-\\theta)\\qquad \\theta = \\frac{\\alpha}{L}$$\n",
    "$h$ is applied pointwise to $y$ and is the shrinkage function. Simplying calculating $x_{t+1} = p_L(x_t)$ is the ISTA algorithm. If we more intelligently choose our new value to probe our function, then we get faster convergence. The FISTA algorithm is as follows:\n",
    "\n",
    "1. Initialize $y_0 = x_0 = X0$, $t_0=1$. \n",
    "\n",
    "2. For $k \\ge 0$, iterate the following:\n",
    "\n",
    "$$x_{k+1} = p_L(y_k)\\qquad t_{k+1} = 0.5 * (1 + \\sqrt{1 + 4 * t_k ^2})\\qquad y_{k+1} = x_{k+1} + \\frac{t_k - 1}{t_{k+1}} * (x_{k+1} - x_k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savemat(os.path.join(output_dir, 'mnist_dictionary.mat'), \n",
    "       {'D': t_D.get_value(), 'Alpha': Alpha,\n",
    "        'Algorithm': 'FISTA', \n",
    "        'Normalization': 'Equal Standard Deviation'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
